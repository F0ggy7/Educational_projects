{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Выполнить стемминг токенизированного текста из Задания 2 с использованием различных стеммеров. Сравнить результаты стемминга, включая время выполнения операции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В задании 2 мы работали с предложениями с 100 по 115. Выделю их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем книгу и записываем ее в переменную.\n",
    "with open('The_Little_Prince.txt') as book:\n",
    "    read_book = book.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убераю из текста обозначение пустой строки.\n",
    "spec_chars = '\\n' \n",
    "book = \"\".join([ch for ch in read_book if ch not in spec_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"That is not an object. It flies. It is an airplane. It is my airplane.\" And I was proud to have him learn that I could fly. He cried out, then: \"What! You dropped down from the sky?\" \"Yes,\" I answered, modestly. \"Oh! That is funny!\" And the little prince broke into a lovely peal of laughter, which irritated me very much. I like my misfortunes to be taken seriously. Then he added: \"So you, too, come from the sky! Which is your planet?\" At that moment I caught a gleam of light in the impenetrable mystery of his presence; and I demanded, abruptly: \"Do you come from another planet?\"'"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sent_tokenize(book, language='english')\n",
    "# Выделю предложения с 100 по 115.\n",
    "proposals = tokens[100: 115] \n",
    "# Занесу все в одну строку.\n",
    "text = ' '.join(proposals)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### После выделения предлодения токенеизрую слова в этих предложениях и удалю стоп слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Токинизируем текс.\n",
    "word_tokens = word_tokenize(text)\n",
    "len(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В тексте часто встречается символ � и машиной он читается как 'пїѕ'. Я рещил его добавить в список стоп слов.\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.append('пїѕ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляю знаки препинания и перевожу все слова в нижний регистр\n",
    "remove_punctuation = str.maketrans('', '', string.punctuation)\n",
    "lower_text = [x for x in [t.translate(remove_punctuation).lower() for t in word_tokens] if len(x) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['object', 'flies', 'airplane', 'airplane', 'proud', 'learn', 'could', 'fly', 'cried', 'dropped', 'sky', 'yes', 'answered', 'modestly', 'oh', 'funny', 'little', 'prince', 'broke', 'lovely', 'peal', 'laughter', 'irritated', 'much', 'like', 'misfortunes', 'taken', 'seriously', 'added', 'come', 'sky', 'planet', 'moment', 'caught', 'gleam', 'light', 'impenetrable', 'mystery', 'presence', 'demanded', 'abruptly', 'come', 'another', 'planet']\n"
     ]
    }
   ],
   "source": [
    "# Удаляю стоп слова\n",
    "tokens_without_stop_w = [word for word in lower_text if not word in all_stopwords]\n",
    "print(tokens_without_stop_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 ['ARLSTem', 'ARLSTem2', 'Cistem', 'ISRIStemmer', 'LancasterStemmer', 'PorterStemmer', 'RSLPStemmer', 'RegexpStemmer', 'SnowballStemmer', 'StemmerI', 'WordNetLemmatizer']\n"
     ]
    }
   ],
   "source": [
    "stem_list = [tkn for tkn in dir(nltk.stem) if tkn[0].isupper()]\n",
    "print(len(stem_list), stem_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PorterStemmer\n",
    "Алгоритм Портера. Алгоритм использует пять фаз сокращения слов, каждая из которых имеет свой собственный набор правил отображения. Porter Stemmer — старейший стеммер, известный своей простотой и скоростью. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['object', 'fli', 'airplan', 'airplan', 'proud', 'learn', 'could', 'fli', 'cri', 'drop', 'sky', 'ye', 'answer', 'modestli', 'oh', 'funni', 'littl', 'princ', 'broke', 'love', 'peal', 'laughter', 'irrit', 'much', 'like', 'misfortun', 'taken', 'serious', 'ad', 'come', 'sky', 'planet', 'moment', 'caught', 'gleam', 'light', 'impenetr', 'mysteri', 'presenc', 'demand', 'abruptli', 'come', 'anoth', 'planet']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(word) for word in tokens_without_stop_w]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SnowballStemmer\n",
    "Snowball Stemmer также разработан Портером. Используемый здесь алгоритм является более точным и известен как «English Stemmer» или «Porter2 Stemmer». Он предлагает небольшое улучшение по сравнению с оригинальным Porter Stemmer как по логике, так и по скорости. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic, danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, porter, portuguese, romanian, russian, spanish, swedish\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['object', 'fli', 'airplan', 'airplan', 'proud', 'learn', 'could', 'fli', 'cri', 'drop', 'sky', 'yes', 'answer', 'modest', 'oh', 'funni', 'littl', 'princ', 'broke', 'love', 'peal', 'laughter', 'irrit', 'much', 'like', 'misfortun', 'taken', 'serious', 'ad', 'come', 'sky', 'planet', 'moment', 'caught', 'gleam', 'light', 'impenetr', 'mysteri', 'presenc', 'demand', 'abrupt', 'come', 'anoth', 'planet']\n"
     ]
    }
   ],
   "source": [
    "stemmer2 = SnowballStemmer(\"english\")\n",
    "singles2 = [stemmer2.stem(word) for word in tokens_without_stop_w]\n",
    "print(singles2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LancasterStemmer\n",
    "Он имеет тенденцию давать результаты с чрезмерным стеммингом. Основы не являются лингвистическими, или они могут не иметь значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['object', 'fli', 'airpl', 'airpl', 'proud', 'learn', 'could', 'fly', 'cri', 'drop', 'sky', 'ye', 'answ', 'modest', 'oh', 'funny', 'littl', 'print', 'brok', 'lov', 'peal', 'laught', 'irrit', 'much', 'lik', 'misfortun', 'tak', 'sery', 'ad', 'com', 'sky', 'planet', 'mom', 'caught', 'gleam', 'light', 'impenet', 'mystery', 'pres', 'demand', 'abrupt', 'com', 'anoth', 'planet']\n"
     ]
    }
   ],
   "source": [
    "stemmer3 = LancasterStemmer()\n",
    "singles3 = [stemmer3.stem(word) for word in tokens_without_stop_w]\n",
    "print(singles3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexpStemmer\n",
    "Стеммер регулярных выражений использует регулярные выражения для идентификации морфологических аффиксов. Любые подстроки, соответствующие регулярным выражениям, будут удалены. Подходит, для приведения слов с одинаковыми окончаниями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['object', 'fli', 'airplan', 'airplan', 'proud', 'learn', 'could', 'fly', 'cried', 'dropped', 'sky', 'y', 'answered', 'modestly', 'oh', 'funny', 'littl', 'princ', 'brok', 'lovely', 'peal', 'laught', 'irritated', 'much', 'lik', 'misfortun', 'taken', 'seriously', 'added', 'com', 'sky', 'planet', 'moment', 'caught', 'gleam', 'light', 'impenetr', 'mystery', 'presenc', 'demanded', 'abruptly', 'com', 'anoth', 'planet']\n"
     ]
    }
   ],
   "source": [
    "# min (int) — минимальная длина строки для основы\n",
    "stemmer4 = RegexpStemmer('ing$|e$|able$|er$|es$', min=2)\n",
    "singles4 = [stemmer4.stem(word) for word in tokens_without_stop_w]\n",
    "print(singles4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стеммеры для других языков\n",
    "Просто пару слов про них. Буквально пару!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Арабский стеммер (ISRI). Главное отличие состоит в том, что стеммер ISRI не использует корневой словарь. Кроме того, если корень не найден, стеммер ISRI возвращает нормализованную форму, а не исходное неизмененное слово."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARLSTem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARLSTem — это упрощенный арабский стеммер, основанный на удалении аффиксов из слова (то есть префиксов, суффиксов и инфиксов). Его оценили и сравнили с несколькими другими стеммерами с использованием параметров Пейса (индекс недостаточности стебля, индекс избыточного стебля и вес стебля), и результаты показали, что ARLSTem является многообещающим и обеспечивает высокие характеристики. Этот стеммер не основан на каком-либо словаре и может эффективно использоваться в режиме онлайн.\n",
    "Стеммер ARLSTem: легкий арабский алгоритм стемминга без словаря. Департамент телекоммуникаций и обработки информации. Университет USTHB, Алжир, Алжир. ARLSTem.stem(token) возвращает арабскую основу для входного токена. ARLSTem Stemmer требует, чтобы все токены были закодированы с использованием кодировки Unicode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARLSTem2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это усовершенствование предыдущего арабского стеммера (ARLSTem). Новую версию сравнивали с исходным алгоритмом и несколькими существующими стеммерами для арабского языка, и результаты показали, что новая версия значительно устраняет ошибки неполного стемминга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cistem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просто стемер для арабского языка \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2. Реализовать собственный стеммер, сравнить результаты его работы с работой стеммеров из библиотеки nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stem(data_word, affix, min_len_word):\n",
    "    stem_data = []\n",
    "    for word in data_word:\n",
    "        f_is = word.endswith(affix)\n",
    "        if f_is == True:\n",
    "            stem_len_word = []\n",
    "            for af in affix:\n",
    "                stem_word = word.removesuffix(af)\n",
    "                if len(stem_word) >= min_len_word:\n",
    "                    stem_len_word.append(stem_word)\n",
    "                else:\n",
    "                    stem_len_word.append(word)\n",
    "            len_word = min(filter(None, stem_len_word), key=len)\n",
    "            stem_data.append(len_word)\n",
    "        else:\n",
    "            stem_data.append(word)\n",
    "    return (stem_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['object', 'flie', 'airplane', 'airplane', 'proud', 'learn', 'could', 'fly', 'cri', 'dropp', 'sky', 'yes', 'answer', 'modestly', 'oh', 'funny', 'little', 'prince', 'broke', 'lovely', 'peal', 'laughter', 'irritat', 'much', 'like', 'misfortune', 'taken', 'seriously', 'add', 'come', 'sky', 'planet', 'moment', 'caught', 'gleam', 'light', 'impenetrable', 'mystery', 'presence', 'demand', 'abruptly', 'come', 'another', 'planet']\n"
     ]
    }
   ],
   "source": [
    "affix = ('ed', 's') # Окончания которые нужно отбросить. Задаются пользователем!\n",
    "min_len_word = 3 # Минимальная длинна основания слова\n",
    "result =  stem(tokens_without_stop_w, affix, min_len_word)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробовал реализовать аналог RegexpStemmer. Впринципе он выполняет все те же функции. Мой вариант не умеет определять лингвистическую целостность, товесть слово может обрезаться и в случае, если затронут корень слова, так же как RegexpStemmer в nltk.  Что касается скорости выполнения, если верить ноутбуку, то она одинаковая. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5886f9edb4e4191e8bfe3159f96c7d2ee4451326d1088b21e8bb1d9fc114d44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
